{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bbadbb6",
   "metadata": {},
   "source": [
    "## Part II: Preparing Text for Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbc7a9d",
   "metadata": {},
   "source": [
    "*Note: This is part of a series on the computational analysis of open-ended survey questions. For part one, on \"Writing Open-Ended Survey Questions for Computational Analysis\" click here.   \n",
    "\n",
    "If you read our first post, you may remember that Evolytics was asked to analyze approximately 68,000 open-ended responses to nine survey questions.  These questions included asking survey participants to list competitor brands they had tried, rate the competitors, and describe their rationale for the rating. \n",
    "\n",
    "In this post, we'll talk about how to prepare text for analysis. The techniques discussed here are general and can apply to any form of text not just survey responses.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f36c52",
   "metadata": {},
   "source": [
    "### Getting Started with Natural Language Processing\n",
    "\n",
    "Prior to preparing our text for analysis, it's important that we define three common terms used in natural language processing. First, a *corpus* is a collection of documents on which we are conducting analysis.  A *document* is any text that is subject to analysis.  This could be a set of reports, social media posts, or, in our cases, open-ended survey responses. Finally, *tokens* are groupings of characters that are meaningful.  Tokens are often words or parts of words.  \n",
    "\n",
    "When preparing a document for analysis we tokenize it, or break it apart into discrete tokens.  In many models, it is the presence and frequency of tokens that characterize a document.  However, not all tokens are informative.  For example, some tokens (e.g., \"a\", \"of\", \"the\") are extremely common and have little purpose other than tying the sentence together grammatically. In linguistics, these are referred to as [function words](https://en.wikipedia.org/wiki/Function_word). Typically, function words are included in a *stopword* list that contains words we wish to exclude from tokenization because they have little value to statistical models.  \n",
    " \n",
    "It is common to [*stem* or *lemmatize*](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) tokens to further standardize them.  Stemming involves the removal of the end of a word to get its root.  For example, \"smarter\" and \"smartest\" become \"smart\".  A drawback to stemming is that it may not return proper words.  For instance, \"accelerating\" becomes \"acceler\".  In contrast, lemmatization attempts to return the base form of a word such as might be found in the dictionary.  Using lemmatization \"women\", \"Woman's\", and \"Womanly\" becomes \"woman\" while \"is\" and \"are\" become \"be\".  However, you must tag a tokens part of speech (i.e., noun, adj, verb, etc..) to know how a given word should be lemmatized. Since stemming and lemmatization accomplish the same thing you should only choose one.  Furthermore, you shouldn't assume that you *have* to stem or lemmatize - try your models without and see how they perform.       \n",
    "\n",
    "Finally, you may wish to remove things such as numbers, punctuation, or URLs.  Doing this can be especially helpful for data drawn from the web that may not conform to standard grammar.  However, let me issue two warnings about applying these functions indiscriminately.  First, altering your text can affect how a parts-of-speech classifier tags a given token which in turn can effect lemmatization.  As a result, I caution against aggressively filtering text before lemmatization although you can certainly do so afterwards.  Second, depending upon your corpus numbers, punctuation, and URLs may be informative.  For example, while often numbers by themselves are not informative, stripping them from your tokens may prevent your from identifying things such as the [unicode for emojis](https://unicode.org/emoji/charts/full-emoji-list.html).    \n",
    "\n",
    "### Tokenizing Documents \n",
    "\n",
    "Below we've defined several useful functions for text cleaning. We're using Python's [Natural Language Toolkit (NLTK)](https://www.nltk.org/) which contains a number of tools and models for text processing.  If you've never used NLTK before, run the first cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8abc4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ssanders/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ssanders/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/ssanders/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ssanders/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/ssanders/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/ssanders/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only run if you haven't previously installed these NLTK tools.\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95dacb2",
   "metadata": {},
   "source": [
    "#### Stemming and Tokenization \n",
    "\n",
    "Now let's talk about what we're doing to tokenize and filter our text.  First, we use the string methods from the standard Python library to put text in lowercase. We can also filter out punctuation and numbers using Python's [str.maketrans( )](https://stackoverflow.com/a/41536036) and translate( ) functions. translate( ) substitutes one character for another and maketrans( ) creates the map between characters used by translate( ).  Here we substitute an empty string for all punctuation and numbers to remove them.  \n",
    "\n",
    "Second, we're using [regular expressions](https://docs.python.org/3/library/re.html) (regex) to find and remove URLs. If you're reading this post, you're likely familiar with regex but if not all you need to know is that it is a way of defining patterns.  Here we're using it remove URLs by searching for a whitespace separated string beginning with \"http\" and substituting an empty string in its place. \n",
    "\n",
    "Third, we're using the WordPunctTokenizer from NLTK that will tokenize text creating a Python list of alphabetic and non-alphabetic tokens. We then can use Python [list comprehensions](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions) to filter the token list to remove single character tokens and tokens that are found in our stopword list.  \n",
    "\n",
    "Finally, we are using the SnowballStemmer from NLTK to stem each token as discussed above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "843358d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize.regexp import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "#Remove urls. \n",
    "def remove_urls(text): \n",
    "    return re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "#Remove punctuation. Note- This leaves a space so it plays nice w/ nltk's stopword list.\n",
    "def remove_punctuation(s):\n",
    "    table = str.maketrans({ch: ' ' for ch in string.punctuation})  # this line determines what the punct. is replaced with.\n",
    "    return s.translate(table)\n",
    "\n",
    "\n",
    "#Remove numbers. \n",
    "def remove_numbers(s): \n",
    "    remove_digits = str.maketrans('', '', string.digits)\n",
    "    return s.translate(remove_digits)\n",
    "\n",
    "\n",
    "# Stems tokens. \n",
    "def stem_tokens(tokens, stemmer=SnowballStemmer(\"english\", ignore_stopwords=True)): \n",
    "    return [stemmer.stem(tkn) for tkn in tokens]\n",
    "\n",
    "\n",
    "#Tokenize texts.  Note- It is possible to comment out steps to change how tokenization occurs. \n",
    "def tokenize(text, stem=False):\n",
    "    text = remove_urls(text) # removes urls \n",
    "    text = remove_numbers(text) # removes numbers \n",
    "    text = text.lower() # sets to lowercase\n",
    "    text = text.replace('-', '') # removes hyphens  \n",
    "    tkns = tokenizer.tokenize(text) # tokenizes text\n",
    "    tkns = [remove_punctuation(tkn).strip() for tkn in tkns] #strips punctuation\n",
    "    \n",
    "    # stems tkns\n",
    "    if stem: \n",
    "        tkns = [tkn for tkn in tkns if tkn not in sw] # filters using stopwords\n",
    "        tkns = [tkn for tkn in tkns if len(tkn) > 1] # no single character tkns\n",
    "        tkns = stem_tokens(tkns)\n",
    "        tkns = [tkn for tkn in tkns if tkn not in sw]\n",
    "    return tkns\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "# Creates stopword list from NLTK.\n",
    "sw = stopwords.words(\"english\") + ['']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b965727",
   "metadata": {},
   "source": [
    "Now we're going to do the actual tokenization.  After tokenization our corpus will be a list of documents with each document being a list of tokens.  Note that we also create a map of our indices.  This enables us to link tokenized documents back to the original, untokenized version if we need to remove a particular document during model building.  We keep track of these positionally and would remove the corresponding index.  Keeping a copy of the original documents enables us to inspect them after creating a topic model, which assist in interpretation, and the indices allow us to join the results of any text analysis back to our parent dataset in a survey for other analyses such as slicing by demographics.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf99ab82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents:  5\n"
     ]
    }
   ],
   "source": [
    "# Toy corpus for our example\n",
    "doc_examples = ['We use data, statistical algorithms, and machine learning to help you make business decisions and targeted digital marketing efforts based on potential outcomes.', \n",
    "                'With propensity modeling, you can predict the likelihood of a visitor, lead, or current customer to perform a certain action on your website (i.e. browse your site, click a CTA, pick up their phone to call).',\n",
    "                'Once you can anticipate future customer and user behavior, you can plan for possible challenges and obstacles you’ll need to help that customer or user overcome.', \n",
    "                'We believe in the power of data to affect change and help make a difference in the world. We focus on web analytics and marketing optimization for business evolution ' \\\n",
    "                'and brand growth. As a full-service data analytics consulting company, we partner with clients to activate their data with best-in-class analytics tool implementation, ' \\\n",
    "                'meaningful insights, and expert training. Founded in 2005, we serve clients across different industries. We get to know your business right away so the consulting help ' \\\n",
    "                'we offer you is catered to your specific needs and goals. We don’t just sell you a service; we engage with your business as a partner. We care about your success, and we ' \\\n",
    "                'want to help your business thrive. ', \n",
    "                \"Evolytics can help you with your data science needs.\"\n",
    "               ]\n",
    "\n",
    "print(\"Number of Documents: \", len(doc_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9754c586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orginal Document:  We use data, statistical algorithms, and machine learning to help you make business decisions and targeted digital marketing efforts based on potential outcomes.\n",
      "\n",
      "\n",
      "Tokenized & Stemmed Document:  ['use', 'data', 'statist', 'algorithm', 'machin', 'learn', 'help', 'make', 'busi', 'decis', 'target', 'digit', 'market', 'effort', 'base', 'potenti', 'outcom']\n",
      "\n",
      "\n",
      "Document Index:  0\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "orig_docs = []\n",
    "doc_index = []\n",
    " \n",
    "for i, d in enumerate(doc_examples):\n",
    "    orig_docs.append(d)\n",
    "    docs.append(tokenize(d, stem=True))\n",
    "    doc_index.append(i)\n",
    "\n",
    "i = 0 \n",
    "print(\"Orginal Document: \", orig_docs[i])\n",
    "print('\\n')\n",
    "print(\"Tokenized & Stemmed Document: \", docs[i])\n",
    "print('\\n')\n",
    "print(\"Document Index: \", doc_index[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c054ce67",
   "metadata": {},
   "source": [
    "#### Parts-of-Speech Tagging, Lemmatization, and Tokenization \n",
    "\n",
    "The above approach works well if you wish to stem tokens but slightly different handling is required for lemmatization.  Specifically, we need to tag the part-of-speech using NLTK's pos_tag method.  Unfortunately, the parts-of-speech tags ([see here for a full list](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)) returned by nlkt.pos_tag are not those accepted by the wordnet lemmatizer.  For example, NLTK returns a tag of \"PRP\" (i.e., personal pronoun) for the work \"We\" but the wordnet lemmitizer needs a more simple \"n\" (i.e., noun) tag.  Therefore, we define a helper function, wordnet_pos, that converts the tag to the correct format.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd309510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence:  ['We', 'use', 'data', ',', 'statistical', 'algorithms', ',', 'and', 'machine', 'learning', 'to', 'help', 'you', 'make', 'business', 'decisions', 'and', 'targeted', 'digital', 'marketing', 'efforts', 'based', 'on', 'potential', 'outcomes', '.']\n",
      "\n",
      "\n",
      "Penn Treebank PoS:  [('We', 'PRP'), ('use', 'VBP'), ('data', 'NNS'), (',', ','), ('statistical', 'JJ'), ('algorithms', 'NN'), (',', ','), ('and', 'CC'), ('machine', 'NN'), ('learning', 'NN'), ('to', 'TO'), ('help', 'VB'), ('you', 'PRP'), ('make', 'VB'), ('business', 'NN'), ('decisions', 'NNS'), ('and', 'CC'), ('targeted', 'JJ'), ('digital', 'JJ'), ('marketing', 'NN'), ('efforts', 'NNS'), ('based', 'VBN'), ('on', 'IN'), ('potential', 'JJ'), ('outcomes', 'NNS'), ('.', '.')]\n",
      "\n",
      "\n",
      "Wordnet PoS:  [('We', 'n'), ('use', 'v'), ('data', 'n'), (',', 'n'), ('statistical', 'a'), ('algorithms', 'n'), (',', 'n'), ('and', 'n'), ('machine', 'n'), ('learning', 'n'), ('to', 'n'), ('help', 'v'), ('you', 'n'), ('make', 'v'), ('business', 'n'), ('decisions', 'n'), ('and', 'n'), ('targeted', 'a'), ('digital', 'a'), ('marketing', 'n'), ('efforts', 'n'), ('based', 'v'), ('on', 'n'), ('potential', 'a'), ('outcomes', 'n'), ('.', 'n')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "i = 0\n",
    "\n",
    "def wordnet_pos(pos):\n",
    "    \"\"\"Converts Penn Treebank PoS to Wordnet PoS\n",
    "    \n",
    "    Arg: \n",
    "        pos(str): Penn treebank PoS tag\n",
    "        \n",
    "    Returns: \n",
    "        str: wordnet PoS tag\n",
    "    \"\"\"\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    # Returns noun if not found to avoid lemmatization error. \n",
    "    return tag_dict.get(pos[0], wordnet.NOUN)\n",
    "    \n",
    "\n",
    "tokenized_sentence = nltk.word_tokenize(doc_examples[i]) #tokenizing sentence \n",
    "print(\"Tokenized Sentence: \", tokenized_sentence)\n",
    "tag_sent = nltk.pos_tag(tokenized_sentence) # tagging pos \n",
    "\n",
    "print('\\n')\n",
    "print(\"Penn Treebank PoS: \", tag_sent)\n",
    "\n",
    "words = [(word[0], wordnet_pos(word[1])) for word in tag_sent] # converting to wordnet pos\n",
    "\n",
    "print('\\n')\n",
    "print(\"Wordnet PoS: \", words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b131c9",
   "metadata": {},
   "source": [
    "You may note that the above output contains punctuation.  After we lemmatize our tokens, we can use some of the general filtering techniques or functions we discussed above to clean our documents and remove punctuation and stopwords. The code below demonstrates this and presents us with a clean set of tokens.  However, depending upon your analysis you may wish to retain your PoS tags.  For example, [named entity recognition](https://en.wikipedia.org/wiki/Named-entity_recognition) (i.e., the identification of a person, location, organization, or product in unstructured text) requires PoS tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "894e7277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Tokens:  ['We', 'use', 'data', 'statistical', 'algorithm', 'machine', 'learning', 'help', 'make', 'business', 'decision', 'targeted', 'digital', 'marketing', 'effort', 'base', 'potential', 'outcome']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "def wordnet_pos(pos):\n",
    "    \"\"\"Converts Penn Treebank PoS to Wordnet PoS\n",
    "    \n",
    "    Arg: \n",
    "        pos(str): Penn treebank PoS tag\n",
    "        \n",
    "    Returns: \n",
    "        str: wordnet PoS tag\n",
    "    \"\"\"\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    # Returns noun if not found to avoid lemmatization errror. \n",
    "    return tag_dict.get(pos[0], wordnet.NOUN)\n",
    "\n",
    "    \n",
    "lemmatizer = WordNetLemmatizer() # initializing lemmatizer\n",
    "tokenized_docs = []\n",
    "\n",
    "for doc in doc_examples: \n",
    "    d = []\n",
    "    sentences = nltk.sent_tokenize(doc) # creates a list of sentences\n",
    "    for sentence in sentences:\n",
    "        tokenized_sentence = nltk.word_tokenize(sentence) #tokenizes sentence\n",
    "        tagged = nltk.pos_tag(tokenized_sentence) # pos tagging\n",
    "        for tkn in tagged: \n",
    "            if (tkn[0] not in sw and tkn[0] not in string.punctuation): #filtering punct & stopwords\n",
    "                lemma_tkn = lemmatizer.lemmatize(word=tkn[0], pos=wordnet_pos(tkn[1])) #lemmatization\n",
    "                d.append(lemma_tkn)\n",
    "    tokenized_docs.append(d)\n",
    "\n",
    "print(\"Lemmatized Tokens: \", tokenized_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39fde9a",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    "In this post, we've defined some basic terms for natural language processing and discussed how to prepare text for analysis using Python's Natural Language ToolKit.  Specifically, we discussed how to tokenize and filter text, how to stem and lemmatize tokens, and how to tag the parts of speech of tokens for subsequent analysis.  Although our context is the analysis of open-ended survey questions, the above techniques will work with any body of text.  \n",
    "\n",
    "In our next post, we'll talk about how to detect duplicate texts and how to do named entity recognition to identify people, locations, and products in text.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686caf51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
