{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed8ab44a",
   "metadata": {},
   "source": [
    "## Part IV: LDA Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbadbb6",
   "metadata": {},
   "source": [
    "*Note: This is part of a series on the computational analysis of open-ended survey questions. You can find the other posts in this series here: \n",
    "- \"Part I: Writing Open-Ended Survey Questions for Computational Analysis\" \n",
    "- \"Part II: Preparing Text for Analysis‚Äù \n",
    "- \"Part III: Detecting Near Duplicate Text & Named Entity Recognition\" \n",
    "\n",
    "If you read our previous posts, you may remember that Evolytics was asked to analyze approximately 68,000 open-ended responses to nine survey questions.  These questions included asking survey participants to list competitor brands they had tried, rate the competitors, and describe their rationale for the rating. They also asked what the consumer liked about the brand and why they would recommend it.  In this post, we'll demonstrate how to create a topic model that summarizes the content of a large number of documents and how you might use this for further analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cb16be",
   "metadata": {},
   "source": [
    "#### Installing Gensim\n",
    "We're going to use the [Gensim](https://radimrehurek.com/gensim/) library to create our topic models.  Gensim has several algorithms and metrics for the evaluation of the models. If you do not have it installed you can run the cell below to install it directly in Anaconda.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4afac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing gensim in case I decide to do some topic modeling. \n",
    "!conda install -c anaconda gensim -y\n",
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafb231a",
   "metadata": {},
   "source": [
    "## What's a topic model anyway? \n",
    "\n",
    "Topic modeling algorithms generate statistical models of a document collection (i.e., corpus) by grouping commonly co-occurring words together so that the themes, or \"topics\", are revealed.  Perhaps the most well-known topic modeling algorithm is Latent Dirichlet Allocation (LDA).  I don't want to seem \"hand wavy\" but there are lots of articles and papers that discuss [how LDA works in detail](https://www.youtube.com/watch?v=DDq3OVp9dNA) and this is more of a practical demonstration.  Suffice to say, it is a Bayesian technique that assumes there are latent (i.e., hidden/unknown) topics that are represented by the corpus's vocabulary (i.e., the set of all modeled terms/tokens drawn from the documents).  In a gross simplification, the algorithm essentially iterates over the documents updating the probability regarding whether a given term belongs to a particular topic on each iteration.     \n",
    "\n",
    "There are two outputs of interest from LDA models.  First, it produces a set of topics which are defined as probability distributions of the corpus's vocabulary.  Each term from the corpus's vocabulary appears in every topic but an analyst would inspect the most probable terms to get some idea regarding the nature of the topic.  Second, LDA topic models produce a document X topic matrix showing the degree to which each topic characterizes a document.  Note that topic models *do not* classify documents!  Instead, it is assumed that documents are a proportion of all the model's topics.  This makes intuitive sense - a survey response might be *both* about a products aesthetics and it's build quality.  LDA represents this mix of topics proportionately.            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffedb4ed",
   "metadata": {},
   "source": [
    "## The Data \n",
    "\n",
    "Topic models work best when you have a large number of sufficiently long documents. In other words, a few strings a notebook cell won't cut it and I can't use our client's survey data for confidentiality reasons.  Therefore, we're going to use a dataset that I collected when I was an academic.  All of the guidance in this post can easily be applied to survey responses or any other body of text.\n",
    "\n",
    "Shortly after the 2016 US election, a concerted disinformation campaign conducted by Russian state via Facebook ads was revealed to the public.   The [US House Intelligence Committee subsequently published ads provided by Facebook](https://intelligence.house.gov/social-media-content/social-media-advertisements.htm) that were determined to have been paid for by Russia.   Although PDFs are ~~evil~~ a horrible format for data scientists, I was able to extract not only the text of the advertisements but also the Russian ad spend, engagement metrics, and targeting data.  The nature of the data is most amenable to a nested structure so I saved it as JSON rather than a flat table. Let's take a quick peek at the targeting data! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b71940b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Age': '18 - 65+',\n",
      " 'Language': 'English (US)',\n",
      " 'Location': 'United States',\n",
      " 'People Who Match': {'Interests': 'Martin Luther King, Jr., Black Power, '\n",
      "                                   'Black Arts Movement, Fight the Power, '\n",
      "                                   'Black Panther Party, Afrocentrism, My '\n",
      "                                   'Black is Beautiful, Black is beautiful, '\n",
      "                                   'Malcolm X or Huff Post Black Voices'},\n",
      " 'Placements': 'Instagram Feed'}\n",
      "{'Age': '14 - 65+',\n",
      " 'And Must Also Match': {'Interests': 'Grooveshark, Last.fm, SoundCloud, Vevo, '\n",
      "                                      'Shazam (service) or Google Play Music'},\n",
      " 'Language': 'English (US)',\n",
      " 'Location': 'United States',\n",
      " 'People Who Match': {'Interests': 'Wounded Warrior Project, Support our '\n",
      "                                   'troops, Veterans, AMVETS, Disabled '\n",
      "                                   'American Veterans, Chris Kyle or '\n",
      "                                   'Supporting Our Veterans'},\n",
      " 'Placements': 'Instagram Feed'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pprint\n",
    "\n",
    "f = open('russian_fb_data_w_engagement.json')\n",
    "data = json.load(f)\n",
    "\n",
    "keys = [k for i, k in enumerate(data)]\n",
    "[pprint.pprint(data[k]['targeting']) for k in keys[1:3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b84c32",
   "metadata": {},
   "source": [
    "A prevailing theory following the 2016 election cycle was that Russia's social media campaign was intended to help Donald Trump.  However, you'll notice that the Russian's were targeting segments of the population that generally leaned both liberal (e.g., black lives matter) and conservative (e.g., military veterans).  One interpretation of this pattern is that rather than trying to help a given candidate, the Russians were attempting to exploit social fractures in our society.  I would suggest the [University of Oxford's report on the 2016 disinformation campaign](https://demtech.oii.ox.ac.uk/wp-content/uploads/sites/93/2018/12/IRA-Report-2018.pdf) if you want additional details on the content and how these posts were identified. We're going to remain apolitical.  \n",
    "\n",
    "Rather than topic modeling the text of the posts, we're going to model the targeting criteria that the Russians used.  We're doing this for three reasons.  First, as previously noted, the Russians were making inflammatory and, in some cases, offensive posts which I don't want to amplify.  Second, using the targeting criteria highlights that we can use topic modeling with data other than traditional \"documents\" (e.g., surveys, webpages, books, reports, social media posts, etc.).  In fact, topic modeling has been used by [biologists to extract features from genome and protein-coding sequences](https://springerplus.springeropen.com/articles/10.1186/s40064-016-3252-8).  Finally, topic modeling works best with a large number of long documents. Here we have a relatively small dataset but the ad targeting data has minimal overlap allowing us to detect better defined topics in small dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d945f36",
   "metadata": {},
   "source": [
    "## Preparing Text for Topic Modeling\n",
    "\n",
    "In previous posts in this series, we discussed strategies for tokenization of text and how to de-duplicate documents.  We're going to use these techniques to prepare our data for analysis.  I will provide a brief description of how and why they're being used but direct you to previous posts for further details. \n",
    "\n",
    "#### Tokenization\n",
    "\n",
    "First, let's extract our targeting criteria from the JSON and tokenize it. As you might have noted, the targeting criteria is a nested dictionary so for each ad we simply get the values of that dictionary and build out a string for our document.  Pooling many short documents is a necessary step when your documents are not sufficiently long for LDA analysis and we can find [examples of this strategy in academic research](https://people.csail.mit.edu/davidam/assets/publications/2016_twitter_topic_modeling/ICWSM2016_Topic.pdf). However, you should have criteria that makes sense regarding what documents to pool.  In this case, any targeting criteria for a given ad is being pooled.    \n",
    "\n",
    "Second, note that we're also creating a mapping of our dictionary keys. This will allow us to join the results of our topic model back to our dataset to use the topic modeling scores in other analyses. We're also keep a copy of the original, untokenized documents that will enable us to inspect them later to assist in interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "978131ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell duplicates a cell in part II and does not need to be included for publication.\n",
    "\n",
    "import re\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize.regexp import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "#Remove urls. \n",
    "def remove_urls(text): \n",
    "    return re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "#Remove punctuation. Note- This leaves a space so it plays nice w/ nltk's stopword list.\n",
    "def remove_punctuation(s):\n",
    "    table = str.maketrans({ch: ' ' for ch in string.punctuation})  # this line determines what the punct. is replaced with.\n",
    "    return s.translate(table)\n",
    "\n",
    "\n",
    "#Remove numbers. \n",
    "def remove_numbers(s): \n",
    "    remove_digits = str.maketrans('', '', string.digits)\n",
    "    return s.translate(remove_digits)\n",
    "\n",
    "\n",
    "# Stems tokens. \n",
    "def stem_tokens(tokens, stemmer=SnowballStemmer(\"english\", ignore_stopwords=True)): \n",
    "    return [stemmer.stem(tkn) for tkn in tokens]\n",
    "\n",
    "\n",
    "#Tokenize texts.  Note- It is possible to comment out steps to change how tokenization occurs. \n",
    "def tokenize(text, stem=False):\n",
    "    text = remove_urls(text) # removes urls \n",
    "    text = remove_numbers(text) # removes numbers \n",
    "    text = text.lower() # sets to lowercase\n",
    "    text = text.replace('-', '') # removes hyphens  \n",
    "    tkns = tokenizer.tokenize(text) # tokenizes text\n",
    "    tkns = [remove_punctuation(tkn).strip() for tkn in tkns] #strips punctuation\n",
    "    \n",
    "    # stems tkns\n",
    "    if stem: \n",
    "        tkns = [tkn for tkn in tkns if tkn not in sw] # filters using stopwords\n",
    "        tkns = [tkn for tkn in tkns if len(tkn) > 1] # no single character tkns\n",
    "        tkns = stem_tokens(tkns)\n",
    "        tkns = [tkn for tkn in tkns if tkn not in sw]\n",
    "    return tkns\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "# Creates stopword list from NLTK.\n",
    "sw = stopwords.words(\"english\") + ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d92bb213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 2195 documents for analysis.\n"
     ]
    }
   ],
   "source": [
    "file_id = [] #keys from JSON \n",
    "docs = []  #tokenized documents. \n",
    "orig_docs = [] #untokenized documents. \n",
    "\n",
    "for k in keys: \n",
    "    targeting = \"\"\n",
    "    \n",
    "    try: \n",
    "        match = data[k]['targeting']['People Who Match']\n",
    "    except: \n",
    "        match = {}\n",
    "    \n",
    "    try: \n",
    "        also_match = data[k]['targeting']['And Must Also Match']\n",
    "    except: \n",
    "        also_match = {}\n",
    "    \n",
    "    for _,v in match.items(): \n",
    "        targeting += v +\" \"\n",
    "    \n",
    "    for _,v in also_match.items(): \n",
    "        targeting += v + \" \"\n",
    "    \n",
    "    if len(targeting) > 10: \n",
    "        docs.append(tokenize(targeting, stem=True))\n",
    "        file_id.append(k)\n",
    "        orig_docs.append(targeting)\n",
    "    \n",
    "print(\"We have {} documents for analysis.\".format(len(docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b471b7",
   "metadata": {},
   "source": [
    "#### De-Duplication\n",
    "I've personally inspected every ad in this dataset and I know that the Russians reused the same targeting criteria for different ads across multiple communities.  That means we have duplicates.  As we discussed in part III of this series, we'll use cosine similarity to find our duplicates and then filter them out.  By specifying the \"tokenizer\" parameter of sci-kit learn's TfidfVectorizer we can use our self-defined tokenizer. This allows us to  take advantage of stemming and lemmatization which aren't supported by sci-kit learn's built-in tokenizer. \n",
    "\n",
    "Once we find our duplicates, we need to filter them from our dataset.   We do this by reverse sorting our list of indices for duplicate documents.  Then we make deletions from the end of our document list moving to the beginning so that we do not affect the indices of unassessed items.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69b0a75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents:  2195\n",
      "Number of Unique Tokens:  787\n",
      "Shape of Document X Term Matrix:  (2195, 787)\n",
      "Shape of Cosine Similiarity Matrix:  (2195, 2195) \n",
      "\n",
      "***Number of Duplicates***\n",
      "1909 documents had duplicates \n",
      "\n",
      "After removing duplicates there are 286 documents\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize)\n",
    "doc_term_matrix = vectorizer.fit_transform(orig_docs)\n",
    "cos_sim_matrix = cosine_similarity(doc_term_matrix)\n",
    "\n",
    "print(\"Number of Documents: \", doc_term_matrix.shape[0]) \n",
    "print(\"Number of Unique Tokens: \", doc_term_matrix.shape[1])\n",
    "print(\"Shape of Document X Term Matrix: \", doc_term_matrix.shape)\n",
    "print(\"Shape of Cosine Similiarity Matrix: \", cos_sim_matrix.shape, \"\\n\")\n",
    "\n",
    "def get_indices(m, i, threshold): \n",
    "    row = m[i]\n",
    "    return [i[0] for i in np.argwhere(row>threshold)]\n",
    "\n",
    "\n",
    "def get_matches(m, index=None, threshold=.5): \n",
    "    \n",
    "    np.fill_diagonal(m, 0)\n",
    "    \n",
    "    if index: \n",
    "        return {f\"Document {index}\": get_indices(m, index, threshold)}\n",
    "    \n",
    "    else: \n",
    "        _,num_rows = m.shape\n",
    "        docs = {}\n",
    "        for j in range(num_rows): \n",
    "            doc = f\"Document {j}\"\n",
    "            indices = get_indices(m, j, threshold)\n",
    "            docs[doc]=indices\n",
    "        \n",
    "        return docs \n",
    "\n",
    "matches = get_matches(cos_sim_matrix, threshold=.9)\n",
    "\n",
    "duplicates = []\n",
    "skip = []\n",
    "\n",
    "for i,k in enumerate(matches.items()): \n",
    "    values = k[1]\n",
    "    if len(values) > 0 and i not in duplicates:\n",
    "        duplicates.extend(values)\n",
    "        \n",
    "        \n",
    "duplicates = list(set(duplicates))\n",
    "duplicates.sort(reverse=True)\n",
    "print(\"***Number of Duplicates***\")\n",
    "print('{} documents had duplicates'.format(len(duplicates)), \"\\n\")\n",
    "\n",
    "for i in duplicates:\n",
    "    del orig_docs[i]\n",
    "    del docs[i]\n",
    "    del file_id[i]\n",
    "    \n",
    "print(\"After removing duplicates there are {} documents\".format(len(docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e04d871",
   "metadata": {},
   "source": [
    "Now that we have a list of tokenized and de-duplicated documents, we're going to add bigrams, or pairs of words that frequently occur together (e.g., gun_control or united_states).  While bigrams are optional, they're provide information that can help distinguish between contexts in which terms are used (e.g., united_states vs. united_airlines).     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4e4048d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bigrams to docs if they appear 10 times or more.\n",
    "from gensim.models import Phrases\n",
    "\n",
    "bigram = Phrases(docs, min_count=10)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09b4557",
   "metadata": {},
   "source": [
    "LDA doesn't care about the order of words or the part of speech it is - it's a bag-of-words model.  It simply wants to know how often a given word appeared in each document.  The code below creates a gensim *dictionary* (**not** a Python dictionary), or a list of all unique terms from our documents.  Next, we filter out words that rarely occur or words that are extremely common.  This is because a term that is in every document (e.g., \"the\") or a term that is unique to a given document doesn't provide any information to help us develop topics across the corpus.   \n",
    "\n",
    "Finally, we create a bag-of-words representation of our documents.  Essentially, we assign each word in the corpus a number and then for each document we count the number of times that term appears.   The code below provides an example of a document.  Each tuple contains integers representing a specific word in the corpus and its frequency in that document.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26717d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 114\n",
      "Number of documents: 286\n",
      "Example of Representation of a Document in Corpus: \n",
      " [(4, 2), (31, 1), (74, 1)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Remove rare and common tokens.\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.8)\n",
    "\n",
    "# Vectorize data.\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))\n",
    "print(\"Example of Representation of a Document in Corpus: \\n\", corpus[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0652fcb3",
   "metadata": {},
   "source": [
    "## Training the Model \n",
    "We are finally ready to train our model using the function below!  I want to point out two things. First, when fitting the model the random_state should be set to \"1\". This is extremely important because it ensures replicability.  If it is unset, you will always get a slightly different model.  Second, LDA requires the number of topics to be specified. Below we'll talk about strategies to determine the number of topics.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24ea1ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA model.\n",
    "def train_lda(num_top=10, corpus=None, d=None): \n",
    "    \"\"\"\n",
    "    Fits an LDA model from gensim. \n",
    "    \n",
    "    Args: \n",
    "        num_top (int): number of topics to which to fit the model. \n",
    "        corpus (lst of tuples):\n",
    "        d (obj): \n",
    "    \n",
    "    \"\"\"\n",
    "    # Set training parameters.\n",
    "    num_topics=num_top\n",
    "    chunksize = 2000\n",
    "    passes = 20\n",
    "    iterations = 400\n",
    "    eval_every = None # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "    # Make a index to word dictionary.\n",
    "    temp = d[0]  # This is only to \"load\" the dictionary.\n",
    "    id2word = d.id2token\n",
    "\n",
    "    # Fit model. Random_state must be set to a constant to ensure replicability\n",
    "    model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \\\n",
    "                           alpha='auto', eta='auto', \\\n",
    "                           iterations=iterations, num_topics=num_topics, \\\n",
    "                           passes=passes, eval_every=eval_every, random_state=1)\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a01d2a",
   "metadata": {},
   "source": [
    "#### How many topics should I have? \n",
    "\n",
    "Topic modeling relies, in part, upon the judgement of the researcher.  In general, you should select the number of topics that provides the greatest human interpretability.  However, it's not always clear the number of topics we should try first.  The [U_MASS measure of topic coherence](http://dirichlet.net/pdf/mimno11optimizing.pdf) considers how often the top words in a topic co-occur across documents. It assumes that random words, or words that do not co-occur with other top words in a topic, are indicators of poor topic coherence.        \n",
    "\n",
    "The reason we created a function for fitting our topic model is so we can loop over it with different parameters.  Here we're fitting models with different numbers of topics and returning the topic coherence score. After calculating our topic coherence scores, we graph them for inspection.  The y axis is the UMASS score and the x axis is the number of topics. There is a bit of art here - you should generally go for a higher UMASS value (i.e., close to zero) but if after inspecting the topics they are difficult to interpret then you should consider other lower scored values.  Additionally, as a rule of thumb, I usually assume there are greater than 5 topics present (otherwise why model?).   Thus, I typically begin with the number of topics equal to or greater than five with the highest topic coherence score. In this case, that is eleven!    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0357e2f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "max_num_topics =20\n",
    "\n",
    "results = []\n",
    "\n",
    "for t in range(5, max_num_topics):\n",
    "    lda_model = train_lda(num_top=t, corpus=corpus, d=dictionary)\n",
    "    corpus_lda = lda_model[corpus]\n",
    "\n",
    "    cm = CoherenceModel(model=lda_model, corpus=corpus_lda, coherence='u_mass')\n",
    "    score = cm.get_coherence()\n",
    "    tup = t, score\n",
    "    results.append(tup)\n",
    "\n",
    "results = pd.DataFrame(results, columns=['topic', 'score'])\n",
    "s = pd.Series(results.score.values, index=results.topic.values)\n",
    "_ = s.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6c73c4",
   "metadata": {},
   "source": [
    "## Evaluating Output\n",
    "\n",
    "### Output 1:  Inspecting Topics Lists\n",
    "\n",
    "Below we create a topic model with eleven topics and then print off the top ten words in each topic.  As an unsupervised learning technique, you should not expect perfect scrutability of the output. Looking the below I might assign the following topic labels: \n",
    "\n",
    "1.  Audio Streaming\n",
    "2.  Donald Trump \n",
    "3.  Facebook Friends of Friends\n",
    "4.  Second Amendment \n",
    "5.  African American Politics \n",
    "6.  Civil Rights Movement \n",
    "7.  Islam \n",
    "8.  Civil Rights Movement (2) \n",
    "9.  US Policing \n",
    "10. LGBT/Liberal Politics \n",
    "11. Black Power Movement \n",
    "\n",
    "Generally, it is helpful to have some domain knowledge.  For example, topic one looks like garbage unless you know that [the Russians were promoting a Chrome extension for audio streaming](https://www.wired.com/story/russia-facebook-ads-sketchy-chrome-extension/) which if installed spammed the Facebook friends of a user with, presumably, propaganda.  The point is that even if a topic appears to not make sense, it usually bears some cursory examination.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82fc8c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:  music, soundcloud, fm, last, facebook_access, browser, access, facebook, grooveshark, grooveshark_last\n",
      "Topic 2:  jesus, trump, donald, donald_trump, presid, parti, conserv, jr, baptism, king\n",
      "Topic 3:  polit, friend_peopl, connect, friend, peopl, polit_conserv, conserv, us, gun, amend\n",
      "Topic 4:  gun, veteran, right, america, amend, second, second_amend, associ, gun_owner, owner\n",
      "Topic 5:  us, veteran, american, african, african_american, polit, black, voic, liber, polit_content\n",
      "Topic 6:  africanamerican, king, martin, martin_luther, luther, king_jr, jr, histori, black, movement\n",
      "Topic 7:  islam, muslim, patriot, american, america, cultur, state, parti, histori, unit\n",
      "Topic 8:  american, african_american, african, movement, us, cultur, nation, civil, histori, right\n",
      "Topic 9:  state, unit, unit_state, polic, america, nation, parti, gun, stop, associ\n",
      "Topic 10:  lgbt, social, communiti, immigr, justic, right, movement, cultur, stop, histori\n",
      "Topic 11:  black, power, histori, cop, panther, beauti, parti, color, panther_parti, block\n"
     ]
    }
   ],
   "source": [
    "from gensim import matutils \n",
    "\n",
    "model = train_lda(num_top=11, corpus=corpus, d=dictionary)\n",
    "\n",
    "# Provides a convenient human readable list of topic terms. \n",
    "for i in range(model.num_topics): \n",
    "    terms = model.get_topic_terms(i, topn=10)\n",
    "    term_lst = []\n",
    "    for j in terms: \n",
    "        term_lst.append(model.id2word[j[0]])\n",
    "    term_lst = (\", \").join(term_lst)\n",
    "    print('Topic {}: '.format(i+1), term_lst)\n",
    "    \n",
    "# Creating an accessible Document X Topic matrix. \n",
    "docTopicProbMat = model[corpus]\n",
    "dtm = matutils.corpus2dense(docTopicProbMat, model.num_topics, len(docTopicProbMat)).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024f21d1",
   "metadata": {},
   "source": [
    "#### Getting a Selection of Documents for Each Topic\n",
    "\n",
    "I've defined some functions that allow us to print a subset of untokenized documents for each topic. Note that you can set the number of documents it returns as well as the threshold for the topic score.  Higher thresholds mean that the document focuses more on a given topic (i.e., it's a purer representation of that topic).  The reason that we do this is to assist in interpretability - it can be easier to spot the commonalities in documents when looking at untokenized documents that are strong matches for a given topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19e9f874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Topic #1 ***\n",
      "\n",
      "African American (US) On Grooveshark, Last.fm, SoundCloud, Vevo, Shazam (service) or Google Play Music US politics (conservative) \n",
      "Hotep, Nelson Mandela, Kemetism, Kwame Nkrumah, Afrocentrism, Marcus Garvey or Nigeria On Grooveshark, Last.fm, SoundCloud, Vevo, Shazam (service) or Google Play Music US politics (conservative) \n",
      "jesus love u or I Have Decided to Follow Jesus On Grooveshark, Last.fm, SoundCloud, Vevo, Shazam (service) or Google Play Music US politics (conservative) \n",
      "Mexico, Latin hip hop, Chicano Movement, Hispanidad, Lowrider or Chicano rap On Grooveshark, Last.fm, SoundCloud, Vevo, Shazam (service) or Google Play Music US politics (conservative) \n",
      "Facebook access (browser): Chrome Grooveshark, Last.fm, SoundCloud, Vevo, Shazam (service) or Google Play Music \n",
      "Interests Free software Facebook access (browser): Chrome \n",
      " Facebook access (browser): Chrome Free software \n",
      "Facebook access (browser): Chrome Music, Last.fm, SoundCloud or Apple Music \n",
      "Facebook access (browser): Chrome Facebook access (browser): Firefox, Facebook access (browser): Chrome or Facebook access (browser): Opera Grooveshark, Last.fm, SoundCloud, Vevo or Google Play Music \n",
      "Interests Music Facebook access (browser): Chrome \n",
      "\n",
      "*** Topic #2 ***\n",
      "\n",
      "Donald Trump for President, Coal Miner \n",
      "Donald Trump, Manufacturing, Donald Trump for President or conservative daily, Coal Miner \n",
      "The Tea Party, Donald Trump, donald j trump, Donald Trump for President, lvanka Trump Fine Jewelry or Donald Trump Jr. \n",
      "Donald Trump for President \n",
      "Tea Party movement, The Tea Party, Tea Party Patriots, Americans for Prosperity, donald j trump, Donald Trump for President or Donald Trump Jr. \n",
      "The Tea Party, Conservative Republicans of Texas or Donald Trump for President \n",
      "Christ's Commission Fellowship, Jesus, Jesus Daily, Knowing Jesus, Walk with Jesus or The Bible (TV miniseries) \n",
      "Donald Trump \n",
      "Donald Trump for Clinton FRAUDation News Feed on mobile President \n",
      "\n",
      "*** Topic #3 ***\n",
      "\n",
      "Friends of people who are connected to Black Matters \n",
      "Friends of people who are connected to Being Patriotic \n",
      "Friends of people who are connected to Williams&Kalvin \n",
      "Friends of people who are connected to Blacktivist \n",
      "Friends of people who are connected to Stop A. 1. \n",
      "Friends of people who are connected to Stop A. I. \n",
      "Friends of people who are connected to Born Liberal \n",
      "Friends of people who are connected to Secured Borders \n",
      "Friends of people who are connected to Heart of Texas \n",
      "Friends of people who are connected to Don't Shoot \n",
      "\n",
      "*** Topic #4 ***\n",
      "\n",
      "Civil and political rights, Human rights, Anarchism, Nonviolence, Stop the War Coalition, Anti-fascism, Pacifism, Peace movement, The Anarchy, World peace, Anti-war movement, Feminism, PeaceOnEarth or Be a Peacekeeper - The World Peacekeepers Movement African American (US) Gun Owners of America Veterans (US) On Right to keep and bear arms, The Second Amendment, National Rifle Association, Self-defense, Second Amendment Sisters, Gun Owners of America, Second Amendment to the United States Constitution, Concealed carry in the United States, Gun Rights, National Association for Gun Rights, Guns & Ammo or Gun Rights Across America, Veteran, African American (US) Likely to engage with political content (liberal) \n",
      "U.S. Military Paralyzed Veterans of America, Iraq and Afghanistan Veterans of America, Vietnam Veterans of America Foundation, Veterans For America, Support our troops, Vietnam Veterans of America, Vietnam Veterans Against the War, Vietnam Vets, Support Our Veterans, Vietnam Veterans Memorial Fund, Disabled American Veterans or Veterans United Network, \n",
      "Texas Nationalist Movement, Hog Hunting Texas Style, Texas Secession, Texas Hog Hunting, Don't Mess with Texas, Texas Gun Owner, Texas Deer Association, Texas Parks and Wildlife - Hunt, Don't Mess with Texas Program, Texas secession movements, Republic of Texas, Houston, Texas, made in texas, Texas Deer Hunting, Texas Born Texas Proud, Daughters of the Republic of Texas, The Texas Huntress, Texas Got It Right, Texas Values, Open Carry Texas, Texas Gun Talk or Keep Texas Working \n",
      "My Black is Beautiful or Black is beautiful African American (US) Gun Owners of America Veterans (US) On Right to keep and bear arms, The Second Amendment, National Rifle Association, Self-defense, Second Amendment Sisters, Gun Owners of America, Second Amendment to the United States Constitution, Concealed carry in the United States, Gun Rights, National Association for Gun Rights, Guns & Ammo or Gun Rights Across America, Veteran, African American (US) Likely to engage with political content (liberal) \n",
      "The Second Amendment; Protect the Second Amendment; Protecting Your Gun Rights; God. Guns, & American Freedom ; Preserve our right to keep and bear arms or Second Amendment Supporters, US politics (conservative) And Must Also Match. \n",
      "Right to keep and bear arms, 2nd Amendment, National Rifle Association , Gun Owners of America, Second Amendment to the United States Constitution, Concealed carry in the United States, Firearm, National Association for Gun Rights or Guns & Ammo \n",
      "Protect the Second Amendment, Gun Owners of America, Concealed carry, Concealed carry in the United States, National Association for Gun Rights, Guns & Ammo or Students for Concealed Carry \n",
      "Proud to be an American Fly the American Flag, America the Beautiful, American Patriots, I Love the USA, Proud to be an American, Patriot Nation, Flag of the United States, American Patriot or American patriotism, African American (US) Gun Owners of America Veterans (US) On Right to keep and bear arms, The Second Amendment, National Rifle Association, Self-defense, Second Amendment Sisters, Gun Owners of America, Second Amendment to the United States Constitution, Concealed carry in the United States, Gun Rights, National Association for Gun Rights, Guns & Ammo or Gun Rights Across America, Veteran, African American (US) Likely to engage with political content (liberal) \n",
      "Iraq and Afghanistan Veterans of America, Vietnam Veterans of America Foundation, Veterans For America, Support our troops, US Military Veterans, Vietnam Veterans of America, Support Our Veterans, Concerned Veterans for America or Supporting Our Veterans \n",
      "Veterans, United States Department of Veterans Affairs, Disabled American Veterans or Supporting Our Veterans \n",
      "\n",
      "*** Topic #5 ***\n",
      "\n",
      "Military, Veterans or Politics \n",
      "African American (US) BlackNews.com or HuffPost Black Voices \n",
      "Likely to engage with political content (liberal) \n",
      "Police, Law enforcement or Police officer African American (US) U.S. Army Reserve, Veterans (US) On BlackNews.com or HuffPost Black Voices Veteran, African American (US) Likely to engage with political content (liberal) \n",
      "State police, Law enforcement in the United States, Police, Sheriffs in the United States, Veterans, Law enforcement or Police officer African American (US) U.S. Army Reserve, Veterans (US) On BlackNews.com or HuffPost Black Voices Veteran, African American (US) Likely to engage with political content (liberal) \n",
      "Flags of the Confederate States of America, Hart of Dixie, Mud & Trucks, Dixie or Redneck Nation African American (US) U.S. Army Reserve, Veterans (US) On BlackNews.com or HuffPost Black Voices Veteran, African American (US) Likely to engage with political content (liberal) \n",
      "African American (US) U.S. Army Reserve, Veterans (US) On BlackNews.com or HuffPost Black Voices Veteran, African American (US) Likely to engage with political content (liberal) \n",
      "Civil law (common law), Administrative law, Civil procedure or Legal education African American (US) U.S. Army Reserve, Veterans (US) On BlackNews.com or HuffPost Black Voices Veteran, African American (US) Likely to engage with political content (liberal) \n",
      "Far-right politics, breitbart, Right-wing politics or Politics, Likely to engage with political content (conservative) African American (US) U.S. Army Reserve, Veterans (US) On BlackNews.com or HuffPost Black Voices Veteran, African American (US) Likely to engage with political content (liberal) \n",
      "Black Tea Patriots, Black Knowledge, BlackNews.com, Black (Color) or HuffPost Black Voices African American (US) U.S. Army Reserve, Veterans (US) On BlackNews.com or HuffPost Black Voices Veteran, African American (US) Likely to engage with political content (liberal) \n",
      "\n",
      "*** Topic #6 ***\n",
      "\n",
      "Black nationalism, Pan-Africanism, Police misconduct, African-American culture, African-American Civil Rights Movement (1954-68), African-American history, Black Consciousness Movement, Martin Luther King III, Stop Police Brutality or Black (Color) \n",
      "Black nationalism, Pan-Africanism, Police misconduct, African-American culture, African-American Civil Rights Movement (1954-68), African-American history, Black Consciousness Movement, Martin Luther King III, Stop Police Brutality, Syria-News, Black (Color), Refugees of the Syrian Civil War or Syria Charity \n",
      "Pan-Africanism, African-American Civil Rights Movement (1954-68), African-American history or Black (Color) \n",
      "Martin Luther King, Jr., African-American culture, African-American Civil Rights Movement (1954-68), African-American history or Malcolm X \n",
      "Martin Luther King, Jr., My Black is Beautiful, African-American culture, African-American Civil Rights Movement (1954--68), African-American history, Black is beautiful or Black Girls Rock! \n",
      "African-American history \n",
      "Martin Luther King, Jr., Pan-Africanism, Kemetism, Afrocentrism, African-American culture, African-American Civil Rights Movement (1954-68), African-American history, Syria-News, Malcolm X, Refugees of the Syrian Civil War or Syria Charity \n",
      "Martin Luther King, Jr., Black history, African- American Civil Rights Movement (1954-68), African-American history, Malcolm X or Black (Color) \n",
      "Martin Luther King, Jr., Pan-Africanism, Kemetism, African-American culture, African-American Civil Rights Movement (1954-68), African-American history or Malcolm X \n",
      "Martin Luther King, Jr., Pan-Africanism, African- American Civil Rights Movement (1954-68), African-American history, The Move or Malcolm X \n",
      "\n",
      "*** Topic #7 ***\n",
      "\n",
      "Muslims Are Not Terrorists, Islam ism or Muslim Brotherhood \n",
      "Islamism, Proud to be A Muslim or Muslim Brotherhood \n",
      "Islam, Quran, Mecca, Muhammad, Ramadan, Prophets and messengers in Islam, Sunnah, Islamism, Glossary of Islam, Sharia, Allah, All-american muslim culture or As-salamu alaykum \n",
      "Muslims Are Not Terrorists or Muslim Brotherhood \n",
      "Zaid Shakir, Muslims for America or Abu Eesa Niamatullah \n",
      "Islam, Quran, Muhammad, State of Palestine, Muslim world, Mosque, Allah, Muslim Brotherhood or All-american muslim culture, St. Edward's University \n",
      "Islam, Muslim world, Islamism or Muslim Brotherhood \n",
      "Islam, Muslims Are Not Terrorists, Islamism, Muslim Brotherhood or ProductiveMuslim \n",
      "Allah Jesus in Islam, Islam, Quran, Twelver. Hasan ibn Ali, Ali al-Asghar ibn Husayn, Umar, Mecca, Umrah, Muhammad, Muhammad al-Bagir, Ramadan, Abu Bakr, Muhammad al-Mandi, Prophets and messengers in Islam, Ya Hussain; Names of God in Islam, Ahl al-Bayt, \"KARBALA IMAM HUSSEIN (A.S) SHRINE\", Fard, Ulama. Sunnah Ummah Caliphate, Mufti, Imam ; Kaaba, Shia Islam ; Abu Talib ibn Abdul-Muttalib, Islamism. Glossary of Islam ; Hadith, Hajj, Zaynab bint Ali, Fiqh; Musa al- Kadhim. Sharia, Sukayna bint Husayn, Mosque, Imam Ali Mosque, Takbir. Hasan al-Askari, Zakat, Shahada, Basmala, Allah, Eid al-Adha, Hijra (Islam). Prophet, Ali. Fatimah, Ja'far al-Sadiq, Husayn ibn Ali, Abbas ibn Ali, Medina, Sheikh, Sakinah (Fatima al-Kubra) bint Husayn, Ali ibn Husayn Zayn al- Abidin, As-salamu alaykum, Abraham in Islam, Eid al-Fitr, Mary in Islam or Ali al-Ridha, \n",
      "Saudi Aramco, Mosque, Muslim, Islam, Saudi Arabia, Quran or Saba University, Arabic, Islamic studies, Sharia, Islamic philosophy or Quran, Mecca, Muhammad, Muhammad al-Baqir, Prophets and messengers in Islam, Muslim world, Muslim Youth, All Pakistan Muslim League, Fiqh, Ana muslim, Sharia, Muslim League (Pakistan), Imam Ali Mosque, Zakat, Muslim Brotherhood, Ja'far al-Sadiq, History of Islam, Medina, As-salamu alaykum or Islam Book, Muslim, Islam, Ulama or Saudi Arabia Islam, Jesus Christ, Saudi Arabia or Quran, \n",
      "\n",
      "*** Topic #8 ***\n",
      "\n",
      "African American (US) \n",
      "National Museum of American History, Stop Racism!!, Malcolm X Memorial Foundation, Maya Angelou, Mumia Abu Jamal, The Raw Story or mother jones, African American (US) \n",
      "Hispanic and latino american culture, Mexico, Being Chicano, Mexican american culture, Hispanic culture, Latino culture, Latin hip hop, Chicano, Chicano Movement, Hispanidad, Mexican Pride, Lowrider, Chicano rap or La Raza \n",
      "Mexican american culture, Hispanic american culture, Chicano, Chicano Movement. Being Latino, Mexican Pride, So Mexican, La Raza or Mexican American Pride \n",
      "Culture of Mexico, Chicano Movement, Chicano rap or La Raza \n",
      "American Indian Movement, Native American Indian Wisdom, Cherokee language or Cherokee Nation \n",
      "African American (US) National Museum of American History, Maya Angelou, Mumia Abu-Jamal, The Raw Story or mother jones, \n",
      "African American (US) National Museum of American History, Maya Angelou, Mumia Abu-Jamal, The Raw Story or motherjones. \n",
      "African American (US) India Cummings. Feed on mobile \n",
      "Hispanic american culture. Chicano Movement or La Raza \n",
      "\n",
      "*** Topic #9 ***\n",
      "\n",
      "Confederate Flag, Flags of the Confederate States of America, confederate states america, United Daughters of the Confederacy, Confederate States of America, Sons of Confederate Veterans, Confederate States Army, Robert E. Lee, Southern United States or Redneck Social Club \n",
      "Understanding racial segregation in the united states \n",
      "Police Brutality is a Crime, Media bias, National Resistance Movement, Police misconduct, Anonymous (group), Police brutality in the United States or Stop Police Brutality \n",
      "Police, Law Enforcement Today, Fallen Police Officers, Wives Behind the Badge, Inc., The Thin Blue Line, PoliceOne.com, Officer Down Memorial Page; National Law Enforcement Officers Memorial; Respect the Thin Blue line.; The Thin Blue Line (emblem). Police Wives Unite. Police officer. National Police Wives Association or Heroes Behind The Badge \n",
      "Police, Police officer or Safety \n",
      "Seaman, Police officer, Polisi militer, Soldado, Retired Police Officer, Officer, Colonel, Major general (United States), Master sergeant, Chief of police, Commander (United States), Sergeant, Police commissioner, Brigadier general, Petty officer, Officer cadet, Sergeant Major of the Army, Lieutenant colonel (United States), Chief petty officer, Lieutenant commander, Squadron leader, 911 Dispatcher or Rear Admiral \n",
      "Security alarm, Police, National security, Security guard, Police officer or Safety \n",
      "Confederate Flag, Confederate States Army, Southern Pride, My Big Redneck Family or Redneck Nation \n",
      "Prison-industrial complex, Corrections Corporation of America, American Correctional Association, Incarceration in the United States, Prisoner, The Inmates, Children Of Inmates, Prison Wives or School-to-prison pipeline \n",
      "United States Holocaust Memorial Museum or Fascism \n",
      "\n",
      "*** Topic #10 ***\n",
      "\n",
      "Rainbow flag (LGBT movement), LGBT rights by country or territory, Gay Rights, Lesbian community, LGBT community, LGBT Equality, Same-sex marriage in the United States, Same-sex marriage or LGBT social movements \n",
      "LGBT community \n",
      "LGBT history, LGBT culture, LGBT community or LGBT social movements \n",
      "Bisexuality, Lesbian community, LGBT culture, LGBT community, Transgenderism or Human Sexuality \n",
      "Human rights or Social justice \n",
      "Bernie Sanders, LGBT rights by country or territory, Lesbian community, LGBT community, Hillary Clinton or Same-sex marriage \n",
      "Gay pride, LGBT culture, LGBT community or LGBT social movements \n",
      "Gay pride, LGBT community, Homosexuality, LGBT culture or Same-sex marriage \n",
      "Stop Illegal Immigration \n",
      "Immigration or Illegal immigration \n",
      "\n",
      "*** Topic #11 ***\n",
      "\n",
      "Black Power, Fight the Power, Black history, My Black is Beautiful, Black History Month, African-American history, Black is beautiful, Malcolm X or Black Girls Rock! \n",
      "Black Power.. Fight the Power, Black Panther Party.. Slavery by Another Name, Malcolm X or Cop Block \n",
      "African American (US) Fight the Power, Black history, My Black is Beautiful, Black is beautiful or Cop Block, \n",
      "Black Power or Black Panther Party \n",
      "Hotep, Pan-Africanism, Black Dresses, Black Enterprise, BLACK MODELS & FASHION or Black (Color) \n",
      "Black Enterprise On Black Economic Empowerment \n",
      "Black nationalism, Black Power, Black Panther Party, Malcolm X or New Black Panther Party \n",
      "Black Power \n",
      "Black Power or Malcolm X \n",
      "Black Power, Black Panther Party or Cop Block \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_indices_for_topic(col, threshold, dtm):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        col (int): Column index. \n",
    "        threshold (float): Value above which indices should be returned.\\\n",
    "        dtm (ndarray): Document X topic matrix.\n",
    "    \n",
    "    Return: \n",
    "        lst of ints: Indices that exceed threshold. \n",
    "    \"\"\"\n",
    "    topic = dtm[:,col]\n",
    "    indices = [i[0] for i in np.argwhere(topic > threshold).tolist()]\n",
    "    return indices    \n",
    "\n",
    "\n",
    "def view_docs_by_topic(dtm, threshold, d, num_return=None):\n",
    "    \"\"\"\n",
    "    Returns a selection of documents which topic scores exceeding threshold. \n",
    "    \n",
    "    Args: \n",
    "        dtm (ndarray): Document X topic matrix.\n",
    "        threshold (float): Topic score threshold above which documents should \n",
    "                           be returned. \n",
    "        d (lst of str): List of untokenized documents.  \n",
    "        num_return (int): Number of documents to return for each topic. \n",
    "    \n",
    "    Returns: \n",
    "        str: Selection of original docs with topic scores exceeding threshold.\n",
    "    \"\"\"\n",
    "    output = \"\"\n",
    "    for i in range(dtm.shape[1]):\n",
    "        indices = get_indices_for_topic(i, threshold, dtm)\n",
    "        selected_docs = [d[j] for j in indices] \n",
    "        output += \"*** Topic #{} ***\\n\\n\".format(i+1)\n",
    "        for s_d in selected_docs[:num_return]: \n",
    "            output += s_d + \"\\n\"\n",
    "        output += \"\\n\"\n",
    "    return output\n",
    "\n",
    "\n",
    "number_of_docs =10\n",
    "threshold = .7\n",
    "print(view_docs_by_topic(dtm, threshold, orig_docs, number_of_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fda54e7",
   "metadata": {},
   "source": [
    "### Output 2: Inspecting the Document X Topic Matrix \n",
    "Remember that each document (i.e., a Facebook ad's targeting criteria) is a mix of all topics in LDA!  This makes intuitive sense.  Afterall, you can target someone how engages with content representing a mix of the topics above (e.g., you can imagine a segment that cares about both US policing and civil rights).  \n",
    "\n",
    "Our Document X Topic matrix (i.e., dtm) has a row for each document (in this case a Facebook ad) and a column for each topic.  The values for a given row represent the percent each topic is present in that document.  Intuitively, if you sum across the topics, you should get approximately 1.00 or 100% (plus or minus some rounding). In contrast, when you sum the columns of the documents (i.e., the topics) it is not immediately interpretable. However, we can convert this to a percent value by dividing it by the number of documents.  This tells us the percentage to which each topic was discussed in the corpus.  This is useful if you want to visualize the prevalence of topics in a document or the corpus.  \n",
    "\n",
    "If you want the number of documents for each topic you must set a threshold and then count the number of documents whose score exceeds it. For example, if you set a value of .33 (i.e., 33 percent allocation to a given topic) then documents can at most be bucketed into 3 topics but probably no more than 2.  However, some documents are never going to reach that threshold for any topic. In other words, the counts will not equal the total of your sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b48422c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note that the row sums are approximately 1: \n",
      " [0.98 0.98 0.98 0.97 1.   0.97 0.95 0.95 0.98 0.94]\n",
      "\n",
      "Note that raw column sums only allow for ordinal ranking: \n",
      " [20.38 13.79 15.39 21.69 39.74 48.78 17.01 23.7  26.83 17.75 32.91]\n",
      "\n",
      "Note that dividing column marginals by the number of documents gives you the topic percentages within the corpus: \n",
      " [0.07 0.05 0.05 0.08 0.14 0.17 0.06 0.08 0.09 0.06 0.12]\n",
      "\n",
      "As expected topic percentages within the corpus sum to ~ 1:  1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Note that the row sums are approximately 1: \\n\", \n",
    "      np.around(dtm.sum(axis=1)[:10], decimals=2))\n",
    "\n",
    "print(\"\\nNote that raw column sums only allow for ordinal ranking: \\n\", \n",
    "     np.around(dtm.sum(axis=0), decimals=2))\n",
    "\n",
    "\n",
    "print(\"\\nNote that dividing column marginals by the number of documents \" \\\n",
    "      \"gives you the topic percentages within the corpus: \\n\" , \n",
    "      np.around(np.true_divide(dtm.sum(axis=0), dtm.shape[0]),\n",
    "      decimals=2))\n",
    "\n",
    "\n",
    "print(\"\\nAs expected topic percentages within the corpus sum to ~ 1: \", \n",
    "     np.around(sum(np.true_divide(dtm.sum(axis=0), sum(dtm.sum(axis=1)))), \n",
    "              decimals=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a466eb",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this post we've demonstrated how to create and evaluate topic models. I've also provided guidance regarding how to interpret the output of such models. However, you can go even further! For example, using our list of dictionary keys we can join this back to the original dataset and inspect the ads start and end dates to learn how the Russian ad targeting evolved over time. We can also look at which topics received the most engagement by users, what the Russian ad spend was on each topic, or even what the ROI was (they spent shockingly little)! And, of course, we can create visualizations for all of this.\n",
    "\n",
    "I haven't forgotten that we started this series explaining how we analyzed open responses to a client‚Äôs survey. To be clear I used similar techniques to do so. Specifically, I pooled similar questions together since open responses are short, I checked for duplicates to eliminate spam, and I used topic coherence to suggest how many topics were present. Our results provided value to our client that they would not have gotten by simply cherry-picking answers from the survey. Not only did they learn thematically what their consumers were saying but they learned the prevalence of those themes in the responses.\n",
    "\n",
    "If you'd like assistance analyzing open-survey responses, consumer social media posts, or any body of text, please feel free to reach out to Evolytics for assistance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
