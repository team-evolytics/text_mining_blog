{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bbadbb6",
   "metadata": {},
   "source": [
    "## Part III : Detecting Near Duplicate Text & Named Entity Recognition\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6dbc7a9d",
   "metadata": {},
   "source": [
    "*Note: This is part of a series on the computational analysis of open-ended survey questions. For part one, on \"Writing Open-Ended Survey Questions for Computational Analysis\" click here and for part two on “Preparing Text for Analysis” click here.   \n",
    "\n",
    "If you read our first or second post, you may remember that Evolytics was asked to analyze approximately 68,000 open-ended responses to nine survey questions.  These questions included asking survey participants to list competitor brands they had tried, rate the competitors, and describe their rationale for the rating. In this post, we'll discuss the why we needed to find near duplicate entries, how we did it, and how we attributed statements in their rating rationales to specific brands. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9c68d0b",
   "metadata": {},
   "source": [
    "## Finding (Near) Duplicate Text \n",
    "There are several reasons why you may wish to begin an analysis by finding near duplicate text. For instance, finding duplicates can be a low effort way of weeding out spam or form letter type responses. Alternatively, you may wish to find the most common responses. Our survey client wanted to know which competitor brands participants listed most frequently. Fortunately, the survey provided a discrete field in which to list each brand name. Unfortunately, people are terrible, terrible spellers. Whether through ignorance or apathy, participants mangled most brand names including our clients! Due to the frequency of misspellings, what we want is to find and group brand names with their misspellings.\n",
    "\n",
    "We're going to show you two ways to find near duplicate text - one for individual tokens (i.e., words) and one for documents.  We'll start with the technique to find near duplicate tokens since that corresponds to our task of grouping and counting misspelled brand names. \n",
    "\n",
    "#### Finding Near Duplicate Tokens\n",
    "\n",
    "Remember that text cleaning is that it's about standardization. In other words, we want to reduce the variation in text such that we can group and count similar items. We can use some simple python methods to do this. The below function clean_text() uses standard python string methods to remove punctuation, format text to lowercase, and remove leading and trailing whitespaces. This is a good start for standardizing one or two word strings like brand names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ca2abcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleans first three elements:\n",
      "Raw Text:  Brand 1\n",
      "Raw Text:        Brand 1.  \n",
      "Raw Text:  #bR@aND 1!\n",
      "\n",
      "\n",
      "Clean Text:  brand 1\n",
      "Clean Text:  brand 1\n",
      "Clean Text:  brand 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cleaning text to make it amenable to analysis \n",
    "import string \n",
    "\n",
    "def clean_text(txt):\n",
    "    \"\"\"\n",
    "    Removes leading & trailing whitespace, formats to lowercase, removes\n",
    "    punctuation.  \n",
    "    \n",
    "    Args: \n",
    "        txt(str): string of text to be formatted. \n",
    "    \n",
    "    Returns: \n",
    "        (str): formatted string of text. \n",
    "    \"\"\"\n",
    "    table = str.maketrans({ch: '' for ch in string.punctuation}) \n",
    "    return txt.translate(table).lower().strip() \n",
    "\n",
    "# some example data\n",
    "examples = [\"Brand 1\", \"      Brand 1.  \", \"#bR@aND 1!\", \"brand 2\",\n",
    "           \"gizmoA\", \"   Giz@mo b\", \"gi!zmo\", \"g1m#o$ c!\", \"gizmo\",\n",
    "           \"widgetA123\", 'widg', '2jwidget', 'widget', 'widget'\n",
    "           ]\n",
    "\n",
    "print(\"Cleans first three elements:\")\n",
    "[print(\"Raw Text: \", i) for i in examples[:3]]\n",
    "print(\"\\n\")\n",
    "[print(\"Clean Text: \", clean_text(i)) for i in examples[:3]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f963a28",
   "metadata": {},
   "source": [
    "Next, we'll define a function, get_near_duplicate_keys(), to help us find the duplicates. Our function input is a python dictionary, created with the Counter class, which has a string (i.e., the brand name) as its key and the string's frequency as its value.  We assume that the most frequently occurring spelling is correct, which is generally a good heuristic. The second value the function takes is a similarity threshold value between 0 (lowest) and 1(highest).  You should experiment with this value to get good performance on your own data. The function returns a dictionary of strings with a list of their near matches.  \n",
    "\n",
    "Let's talk about what this function is doing.  First, it creates a list of all our tokens (i.e., our brand names) and iterates over them. The first token, by definition, is new and it is impossible for the function to have seen a similar token before.  Therefore, it adds this token to a dictionary with the token as the key and an empty list as the value.  On the next iteration, a similarity score is calculated between the second token and the first.  If computed score exceeds our threshold value, we append the token to the first token's list.  Otherwise, we add the second token to our dictionary as a key with an empty list as a value. This continues until we have iterated through the entire list of tokens.  Although the above approach is not particularly computationally efficient, it's fine for survey data which is simply not that large.   \n",
    "\n",
    "Our function's returns a dictionary with a list of near matches as it's values and our best guess regarding the correctly spelled brand name as the key.  By referring to our original counter dictionary, we can sum the values of all near matches to find out which brands were mentioned most frequently.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb3f3476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Counted Example Tokens***\n",
      "brand 1 : 3\n",
      "brand 2 : 1\n",
      "gizmoa : 1\n",
      "gizmo b : 1\n",
      "gizmo : 2\n",
      "g1mo c : 1\n",
      "widgeta123 : 1\n",
      "widg : 1\n",
      "2jwidget : 1\n",
      "widget : 2\n",
      "\n",
      "\n",
      "***Near Duplicates***\n",
      "brand 1 : ['brand 2']\n",
      "gizmo : ['gizmoa', 'gizmo b', 'g1mo c']\n",
      "widget : ['widgeta123', 'widg', '2jwidget']\n",
      "\n",
      "\n",
      "***Counted Examples Collapsed w/ Near Duplicates***\n",
      "brand 1 : 4\n",
      "gizmo : 5\n",
      "widget : 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import difflib\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def get_near_duplicate_keys(d, threshold =.8): \n",
    "    \"\"\"\n",
    "    Args: \n",
    "        d(dict): dictionary of word sorted by counts (i.e., the value) \n",
    "        threshold(flt): similiarity threshold between 0 and 1. \n",
    "    \n",
    "    Returns\n",
    "        key_mapping(dict): Uses key w/ highest value and has list of near\n",
    "                        duplicate keys. \n",
    "    \"\"\"\n",
    "    \n",
    "    try: # deletes \"nan\" which represents no input in pandas.\n",
    "        del d['nan']\n",
    "    except: \n",
    "        pass \n",
    "    \n",
    "    \n",
    "    key_mapping = {}   # initilizing an empty dict for our keys. \n",
    "    key_list =  list(d.keys())  # making a list of dict keys. \n",
    "\n",
    "    for i in key_list:   #iterating over keys.\n",
    "        if i not in key_mapping: \n",
    "            matched = False # if key is not in dict there is no match. \n",
    "            for k,v in key_mapping.items():  \n",
    "                score = difflib.SequenceMatcher(None, i, k).ratio() # we compare the key to other previous keys.\n",
    "                if score >=threshold:\n",
    "                    key_mapping[k].append(i) # if similiarity exceeds threshold we map it to the appropriate dict key.\n",
    "                    matched = True \n",
    "                    break \n",
    "            if not matched: \n",
    "                key_mapping[i] = []  #since the above is ordered by freq cnt & this is the first time we've encountered a simliair key. \n",
    "\n",
    "    return key_mapping\n",
    "\n",
    "\n",
    "# Cleaning our Examples\n",
    "clean_examples = [clean_text(t) for t in examples]\n",
    "\n",
    "# Counting up our examples. \n",
    "counted_examples = Counter(clean_examples)\n",
    "print(\"***Counted Example Tokens***\")\n",
    "[print(k,\":\", v) for k,v in counted_examples.items()]\n",
    "print('\\n')\n",
    "\n",
    "#Sorting our counter as most common will be considered correct spelling\n",
    "counted_examples = dict(counted_examples.most_common())\n",
    "\n",
    "# Finding our near duplicates \n",
    "near_dupes = get_near_duplicate_keys(counted_examples, threshold = .50) # run function on dict of brand names.\n",
    "print(\"***Near Duplicates***\")\n",
    "[print(k,\":\", v) for k,v in near_dupes.items()]\n",
    "\n",
    "\n",
    "# Counting up our near duplicates\n",
    "counted_examples_collapsed = {}\n",
    "for k,v in near_dupes.items(): \n",
    "    counted_examples_collapsed[k] = counted_examples[k]\n",
    "    for i in v: \n",
    "        counted_examples_collapsed[k] += counted_examples[i]\n",
    "\n",
    "print('\\n')\n",
    "print('***Counted Examples Collapsed w/ Near Duplicates***')\n",
    "[print(k,\":\", v) for k,v in counted_examples_collapsed.items()]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc0c95c7",
   "metadata": {},
   "source": [
    "#### Calculating Document Similiarity\n",
    "\n",
    "You may want to view responses to survey questions that are like one another during qualitative inspection of your data.  Alternatively, you may want to reduce the size of your data by filtering out redundant responses.  A solution is to calculate document similarity.   \n",
    "   \n",
    "The general approach to document similarity is as follows.  First, the documents are tokenized. If you're a bit fuzzy on tokenization you may wish to read the second post in this blog series.  Next, we will vectorize each document.  This can be as simple counting how many times in a document a given token appears for each token in our corpus. A better option is [*Term Frequency-Inverse Document Frequency* (TF-IDF)](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) which weights the frequency with which a term occurs by the inverse of the document frequency (i.e., how many documents in which the term appears). As a result, TF-IDF gives more weight to words that are important and distinctive within a document.  Finally, we compute a similarity metrics between each documents vector to create a matrix of similarity scores.  A good option for this is *cosine similarity*. Without getting too far into the weeds, cosine similarity scores range between 0 (lowest) and 1 (highest) and it is [conceptually related to the Pearson Correlation](https://brenocon.com/blog/2012/03/cosine-similarity-pearson-correlation-and-ols-coefficients/).      \n",
    "\n",
    "If this sounds like a lot of work, then it's fortunate that [sci-kit learn](https://scikit-learn.org/stable/) contains most of what we need. We begin by initializing a [Tfidfvectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).  Calling the .fit_transform( ) method with a corpus both tokenizes and then vectorizes the documents using Tf-Idf score and outputs a document X term (i.e., tokens) matrix.  We can then pass this matrix to sci-kit learn's cosine similarity function to calculate our similarity scores.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f9c579a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents:  6\n",
      "Number of Unique Tokens:  68\n",
      "Shape of Document X Term Matrix:  (6, 68)\n",
      "\n",
      "\n",
      "[[1.         0.07513442 0.17156885 0.08527618 0.10476091 0.09706002]\n",
      " [0.07513442 1.         0.14814629 0.15789534 0.08892584 0.17971401]\n",
      " [0.17156885 0.14814629 1.         0.12761192 0.15676993 0.1452459 ]\n",
      " [0.08527618 0.15789534 0.12761192 1.         0.53381905 0.61900007]\n",
      " [0.10476091 0.08892584 0.15676993 0.53381905 1.         0.60758453]\n",
      " [0.09706002 0.17971401 0.1452459  0.61900007 0.60758453 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Toy corpus for our example\n",
    "doc_examples = ['We use data, statistical algorithms, and machine learning to help you make business decisions and targeted digital marketing efforts based on potential outcomes.', \n",
    "                'With propensity modeling, you can predict the likelihood of a visitor, lead, or current customer to perform a certain action on your website (i.e. browse your site, click a CTA, pick up their phone to call).',\n",
    "                'Once you can anticipate future customer and user behavior, you can plan for possible challenges and obstacles you’ll need to help that customer or user overcome.', \n",
    "                \"Evolytics can help you with your data science needs.\", \n",
    "                \"Evolytics can help you with data visualization.\",\n",
    "                \"Evolytics can help you with your data analysis.\"\n",
    "               ]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "doc_term_matrix = vectorizer.fit_transform(doc_examples)\n",
    "\n",
    "print(\"Number of Documents: \", doc_term_matrix.shape[0]) \n",
    "print(\"Number of Unique Tokens: \", doc_term_matrix.shape[1])\n",
    "print(\"Shape of Document X Term Matrix: \", doc_term_matrix.shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(cosine_similarity(doc_term_matrix))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b27a145f",
   "metadata": {},
   "source": [
    "Now we have a matrix of similarity scores but we still need to be able to retrieve similar documents!  First, we replace all values along the matrix diagonal with zero because these values represent a comparison of a document to itself and will, therefore, always be 1.  Next, for each row we use numpy's [argwhere( )](https://numpy.org/doc/stable/reference/generated/numpy.argwhere.html) method which can return the indices of documents whose similarity score exceeds our provided similarity threshold.  Below I've defined a pair of functions to make this easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25fd676b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Matches for a Specific Document***\n",
      "{'Document 4': [3, 5]}\n",
      "\n",
      "\n",
      "***Matches for All Documents***\n",
      "{'Document 0': [], 'Document 1': [], 'Document 2': [], 'Document 3': [4, 5], 'Document 4': [3, 5], 'Document 5': [3, 4]}\n",
      "\n",
      "\n",
      "***Example of Matching Documents***\n",
      "Evolytics can help you with data visualization.\n",
      "Evolytics can help you with your data science needs.\n",
      "Evolytics can help you with your data analysis.\n"
     ]
    }
   ],
   "source": [
    "# Finding matches \n",
    "\n",
    "cos_sim_matrix = cosine_similarity(doc_term_matrix)\n",
    "\n",
    "def get_indices(m, i, threshold): \n",
    "    row = m[i]\n",
    "    return [i[0] for i in np.argwhere(row>threshold)]\n",
    "\n",
    "\n",
    "def get_matches(m, index=None, threshold=.5): \n",
    "    \n",
    "    np.fill_diagonal(m, 0)\n",
    "    \n",
    "    if index: \n",
    "        return {f\"Document {index}\": get_indices(m, index, threshold)}\n",
    "    \n",
    "    else: \n",
    "        _,num_rows = m.shape\n",
    "        docs = {}\n",
    "        for j in range(num_rows): \n",
    "            doc = f\"Document {j}\"\n",
    "            indices = get_indices(m, j, threshold)\n",
    "            docs[doc]=indices\n",
    "        \n",
    "        return docs \n",
    "\n",
    "print(\"***Matches for a Specific Document***\")\n",
    "print(get_matches(cos_sim_matrix, index = 4))\n",
    "print('\\n')\n",
    "\n",
    "print(\"***Matches for All Documents***\")\n",
    "matches = get_matches(cos_sim_matrix)\n",
    "print(matches)\n",
    "print('\\n')\n",
    "\n",
    "print(\"***Example of Matching Documents***\")\n",
    "print(doc_examples[4])\n",
    "for i in matches[\"Document 4\"]: \n",
    "    print(doc_examples[i]) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d00237f9",
   "metadata": {},
   "source": [
    "### Named Entity Recognition \n",
    "\n",
    "The final thing we will talk about in this post is named entity recognition.  Our survey client had only one response field for customers to explain their ratings for multiple competitor brands.  The challenge was figuring out how to attribute each sentence in the rating explanation to a given brand.  We were able to simply use the similarity methods above to iterate through the sentences in the responses to find our brand names and make a guess about attribution.  But what would you do if you didn't have a list of brands or products that you were looking for?     \n",
    "\n",
    "[Named Entity Recognition](https://www.nltk.org/book/ch07.html#named-entity-recognition) uses part-of-speech tagging to detect proper names, places, organizations, products, and brands.  It does this by examining the structure of a given token in relation to others.  NER enables you to identify entities that might be brands or products.  Below is a code snippet that extracts named entities from an example bio paragraph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "446f7ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sanders',\n",
       " 'University',\n",
       " 'Louisville',\n",
       " 'Evolytics',\n",
       " 'Parkville',\n",
       " 'Missouri',\n",
       " 'Anna',\n",
       " 'University Health',\n",
       " 'Kansas City']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code from https://stackoverflow.com/questions/24398536/named-entity-recognition-with-regular-expression-nltk\n",
    "\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "\n",
    "def get_continuous_chunks(text):\n",
    "    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "    prev = None\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for i in chunked:\n",
    "        if type(i) == Tree:\n",
    "            current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "        elif current_chunk:\n",
    "            named_entity = \" \".join(current_chunk)\n",
    "            if named_entity not in continuous_chunk:\n",
    "                continuous_chunk.append(named_entity)\n",
    "                current_chunk = []\n",
    "        else:\n",
    "            continue\n",
    "    if current_chunk:\n",
    "        named_entity = \" \".join(current_chunk)\n",
    "        if named_entity not in continuous_chunk:\n",
    "            continuous_chunk.append(named_entity)\n",
    "            current_chunk = []\n",
    "    return continuous_chunk\n",
    "\n",
    "\n",
    "text = \"Dr. Sanders taught communication technologies, marketing \" \\\n",
    "       \"research and digital marketing at the University of Louisville. \" \\\n",
    "       \"Currently he works at Evolytics in Parkville, Missouri. His wife, \" \\\n",
    "       \"Anna, is a resident ER physician at University Health in Kansas City, \"\\\n",
    "       \"Missouri.\"\n",
    "\n",
    "get_continuous_chunks(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a5cd5fb",
   "metadata": {},
   "source": [
    "### Conclusion \n",
    "\n",
    "In this blog post we demonstrated how we solved our survey client's problem regarding how to count misspelled competitor brands listed by consumers in an open response survey.  We also showed how to calculate and retrieve similar survey responses by calculating document similarity.  Finally, we illustrated how to use named entity recognition to create a list of people, brands, products, and organizations that are being discussed in a text.  \n",
    "\n",
    "In our final post in this series, we'll discuss topic modeling which can create a statistical model of a large body of documents to discern underlying topic trends.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
